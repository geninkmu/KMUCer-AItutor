import os
import streamlit as st
import pdfplumber
from pptx import Presentation
import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI

# åˆå§‹åŒ– OpenAI API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("å°šæœªè¨­å®š OPENAI_API_KEYã€‚è«‹åœ¨æœ¬æ©Ÿä»¥ç’°å¢ƒè®Šæ•¸æˆ–åœ¨éƒ¨ç½²å¹³å°çš„ Secrets è¨­å®šã€‚")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# åˆå§‹åŒ– ChromaDB
chroma_client = chromadb.PersistentClient(path="index")
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key=OPENAI_API_KEY,
    model_name="text-embedding-3-small"
)
collection = chroma_client.get_or_create_collection(
    name="kmucer",
    embedding_function=openai_ef
)

# è¼”åŠ©å‡½æ•¸ï¼šè®€å– PDF
def read_pdf(file):
    text = ""
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text += page.extract_text() or ""
    return text

# è¼”åŠ©å‡½æ•¸ï¼šè®€å– PPTX
def read_pptx(file):
    text = ""
    prs = Presentation(file)
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text += shape.text + "\n"
    return text

# Streamlit ä»‹é¢
st.title("ğŸ“š KMUCer åŠ©æ•™ (ChromaDB ç‰ˆ)")

menu = st.sidebar.radio("é¸å–®", ["ğŸ’¬ åŠ©æ•™å°è©±", "ğŸ§± å»ºç«‹çŸ¥è­˜åº«"])

if menu == "ğŸ§± å»ºç«‹çŸ¥è­˜åº«":
    st.header("å»ºç«‹ / æ›´æ–°æ•™æçŸ¥è­˜åº«")
    uploaded_files = st.file_uploader("ä¸Šå‚³æ•™ææª”æ¡ˆ (PDF æˆ– PPTX)", type=["pdf", "pptx"], accept_multiple_files=True)
    if st.button("ğŸš€ å»ºç«‹ç´¢å¼•") and uploaded_files:
        for file in uploaded_files:
            if file.name.endswith(".pdf"):
                text = read_pdf(file)
            else:
                text = read_pptx(file)
            # åˆ‡å‰²æ–‡å­—æˆ chunk
            chunks = [text[i:i+500] for i in range(0, len(text), 500)]
            for idx, chunk in enumerate(chunks):
                collection.add(
                    documents=[chunk],
                    metadatas=[{"source": file.name}],
                    ids=[f"{file.name}_{idx}"]
                )
        st.success("ç´¢å¼•å»ºç«‹å®Œæˆï¼")

elif menu == "ğŸ’¬ åŠ©æ•™å°è©±":
    st.header("å‘ KMUCer æå•")
    query = st.text_input("è¼¸å…¥ä½ çš„å•é¡Œï¼š")
    if st.button("é€å‡ºå•é¡Œ") and query:
        results = collection.query(query_texts=[query], n_results=3)
        context_chunks = [doc for doc in results["documents"][0]]
        context_text = "\n".join(context_chunks)

        system_prompt = """ä½ æ˜¯ KMUCerï¼Œé«˜é›„é†«å­¸å¤§å­¸é†«è—¥æš¨æ‡‰ç”¨åŒ–å­¸ç³»çš„èª²ç¨‹åŠ©æ•™ã€‚
        ä½ æ˜¯ç¢©å£«ç­å­¸å§Šï¼Œé¢¨æ ¼ã€Œå°ˆæ¥­ + è¦ªåˆ‡ + æç¬‘ã€ã€‚
        è«‹åŸºæ–¼ä»¥ä¸‹æ•™æå…§å®¹å›ç­”å­¸ç”Ÿå•é¡Œï¼š
        {context}
        """.format(context=context_text)

        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ]
        )

        answer = completion.choices[0].message.content
        st.markdown("### ğŸ—¨ï¸ KMUCer å›ç­”")
        st.write(answer)
        st.markdown("### ğŸ“ åƒè€ƒæ•™æ")
        st.write(results["metadatas"][0])
