
import os, io, time, json, pickle
import streamlit as st
import numpy as np
import chromadb
from chromadb.utils import embedding_functions
import pdfplumber
from pptx import Presentation
from openai import OpenAI

# ---------- Helpers ----------
def clean_text(txt: str) -> str:
    txt = txt.replace("\x00", "").replace("\ufeff", "")
    return "\n".join([line.strip() for line in txt.splitlines() if line.strip()])

def chunk_text(text: str, chunk_size: int = 800, overlap: int = 150):
    text = text.strip()
    if not text:
        return []
    chunks = []
    start = 0
    while start < len(text):
        end = min(len(text), start + chunk_size)
        chunks.append(text[start:end].strip())
        if end == len(text):
            break
        start = max(0, end - overlap)
    return chunks

def extract_from_pdf_bytes(name: str, file_bytes: bytes):
    out = []
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            try:
                text = page.extract_text() or ""
            except Exception:
                text = ""
            text = clean_text(text)
            if text:
                out.append({"source": name, "loc": f"p.{i}", "text": text})
    return out

def extract_from_pptx_bytes(name: str, file_bytes: bytes):
    out = []
    prs = Presentation(io.BytesIO(file_bytes))
    for i, slide in enumerate(prs.slides, start=1):
        texts = []
        for shape in slide.shapes:
            if hasattr(shape, "text") and shape.text:
                texts.append(shape.text)
        if texts:
            text = clean_text("\n".join(texts))
            if text:
                out.append({"source": name, "loc": f"slide {i}", "text": text})
    return out

def embed_texts(client, texts, model: str):
    vecs = []
    # batch to be efficient and avoid rate limits
    BATCH = 64
    for i in range(0, len(texts), BATCH):
        batch = texts[i:i+BATCH]
        resp = client.embeddings.create(model=model, input=batch)
        for d in resp.data:
            vecs.append(d.embedding)
    X = np.array(vecs, dtype="float32")
    faiss.normalize_L2(X)
    return X

def build_index(client, docs, chunk_size=800, overlap=150, model="text-embedding-3-small"):
    chunks, meta = [], []
    for rec in docs:
        for j, ch in enumerate(chunk_text(rec["text"], chunk_size, overlap)):
            chunks.append(ch)
            meta.append({"source": rec["source"], "loc": rec["loc"], "chunk_id": j})
    if not chunks:
        return None, None, None
    X = embed_texts(client, chunks, model=model)
    index = faiss.IndexFlatIP(X.shape[1])
    index.add(X)
    return index, chunks, meta

def embed_query(client, text, model="text-embedding-3-small"):
    resp = client.embeddings.create(model=model, input=[text])
    v = np.array(resp.data[0].embedding, dtype="float32")
    faiss.normalize_L2(v.reshape(1, -1))
    return v

def retrieve(client, index, store, query, top_k=4, model="text-embedding-3-small"):
    qv = embed_query(client, query, model=model)
    D, I = index.search(qv.reshape(1, -1), top_k)
    hits = []
    for rank, idx in enumerate(I[0]):
        hits.append({
            "rank": rank+1,
            "score": float(D[0][rank]),
            "text": store["chunks"][idx],
            "meta": store["meta"][idx],
        })
    return hits

def load_system_prompt():
    try:
        with open("prompts/kmucer_system_prompt.txt", "r", encoding="utf-8") as f:
            return f.read()
    except:
        return "ä½ æ˜¯ KMUCerï¼Œè«‹ç”¨å°ˆæ¥­ä¸”è¦ªåˆ‡çš„èªžæ°£å›žç­”å­¸ç”Ÿå•é¡Œã€‚"

# ---------- Streamlit UI ----------
st.set_page_config(page_title="KMUCer â€” åˆ†æžåŒ–å­¸ AI åŠ©æ•™", page_icon="ðŸ“š", layout="wide")
st.title("ðŸ“š KMUCer â€” åˆ†æžåŒ–å­¸ AI åŠ©æ•™ï¼ˆç„¡éœ€å‘½ä»¤åˆ—ï¼‰")
st.caption("è€å¸«ä¸Šå‚³æ•™æ â†’ ä¸€éµå»ºç«‹çŸ¥è­˜åº« â†’ å­¸ç”Ÿå³å¯é–‹å§‹å°è©±ï¼ˆé™„å¹½é»˜å­¸å§Šæ¨¡å¼ & è€ƒå‰è¡åˆºï¼‰")

with st.sidebar:
    st.subheader("ðŸ”‘ OpenAI è¨­å®š")
    api_key = st.text_input("OpenAI API Key", type="password", help="ä¸æœƒé›¢é–‹æ­¤é é¢ï¼›åªç”¨æ–¼åµŒå…¥èˆ‡å›žç­”ã€‚")
    model = st.selectbox("å°è©±æ¨¡åž‹", ["gpt-4o", "gpt-4o-mini"], index=0)
    embed_model = st.selectbox("åµŒå…¥æ¨¡åž‹ï¼ˆå»ºç«‹ç´¢å¼•ç”¨ï¼‰", ["text-embedding-3-small", "text-embedding-3-large"], index=0)
    humor = st.toggle("å¹½é»˜æ¨¡å¼", value=True)
    exam_mode = st.toggle("è€ƒå‰è¡åˆºæ¨¡å¼ï¼ˆé™„å°æŠ„ï¼‰", value=False)
    top_k = st.slider("æª¢ç´¢æ®µè½æ•¸", 2, 8, 4)
    temperature = st.slider("å‰µé€ åŠ› (temperature)", 0.0, 1.2, 0.7, 0.1)

if not api_key:
    st.warning("è«‹åœ¨å·¦å´è¼¸å…¥ OpenAI API Key æ‰èƒ½ä½¿ç”¨ã€‚")
    st.stop()

client = OpenAI(api_key=api_key)
system_prompt = load_system_prompt()

tab_chat, tab_build = st.tabs(["ðŸ’¬ KMUCer åŠ©æ•™", "ðŸ§± å»ºç«‹çŸ¥è­˜åº«ï¼ˆè€å¸«ç”¨ï¼‰"])

# -------- Build tab --------
with tab_build:
    st.subheader("ä¸Šå‚³æ•™ææª”æ¡ˆ")
    files = st.file_uploader("æ”¯æ´ PDF / PPTX / TXTï¼ˆå¯å¤šæª”ï¼‰", type=["pdf", "pptx", "txt", "md"], accept_multiple_files=True)
    chunk_size = st.number_input("åˆ†æ®µå¤§å°ï¼ˆå­—æ•¸ï¼‰", min_value=400, max_value=1600, value=800, step=50)
    overlap = st.number_input("åˆ†æ®µé‡ç–Šï¼ˆå­—æ•¸ï¼‰", min_value=50, max_value=400, value=150, step=10)
    build_btn = st.button("ðŸš€ å»ºç«‹ / é‡å»ºç´¢å¼•", type="primary")

    if "store" not in st.session_state:
        st.session_state.store = None
    if "index" not in st.session_state:
        st.session_state.index = None

    if build_btn:
        if not files:
            st.error("è«‹å…ˆä¸Šå‚³è‡³å°‘ä¸€å€‹æ•™ææª”ã€‚")
        else:
            docs = []
            with st.spinner("æŠ½å–æ–‡å­—ä¸­â€¦"):
                for f in files:
                    ext = os.path.splitext(f.name)[1].lower()
                    try:
                        if ext == ".pdf":
                            docs.extend(extract_from_pdf_bytes(f.name, f.read()))
                        elif ext == ".pptx":
                            docs.extend(extract_from_pptx_bytes(f.name, f.read()))
                        elif ext in [".txt", ".md"]:
                            txt = f.read().decode("utf-8", errors="ignore")
                            txt = clean_text(txt)
                            if txt:
                                docs.append({"source": f.name, "loc": "full", "text": txt})
                        else:
                            st.warning(f"ä¸æ”¯æ´çš„æ ¼å¼ï¼š{f.name}")
                    except Exception as e:
                        st.warning(f"æŠ½å–å¤±æ•— {f.name}: {e}")

            if not docs:
                st.error("æ²’æœ‰æŠ½å–åˆ°ä»»ä½•æ–‡å­—ï¼Œå¯èƒ½æ˜¯æŽƒæåž‹ PDFï¼ˆéœ€å…ˆ OCRï¼‰æˆ–ç°¡å ±æ˜¯èˆŠç‰ˆ .pptã€‚")
            else:
                with st.spinner("å»ºç«‹å‘é‡ç´¢å¼•ä¸­ï¼ˆéœ€è¦ä¸€é»žæ™‚é–“ï¼‰â€¦"):
                    index, chunks, meta = build_index(client, docs, chunk_size, overlap, model=embed_model)
                if index is None:
                    st.error("å»ºç«‹ç´¢å¼•å¤±æ•—ã€‚")
                else:
                    st.session_state.index = index
                    st.session_state.store = {"chunks": chunks, "meta": meta}
                    st.success(f"ç´¢å¼•å®Œæˆï¼å…±å»ºç«‹ {len(chunks)} å€‹æ®µè½ã€‚")

    if st.session_state.store:
        st.info(f"ç›®å‰ç´¢å¼•æ®µè½æ•¸ï¼š{len(st.session_state.store['chunks'])}")

# -------- Chat tab --------
with tab_chat:
    if "index" not in st.session_state or st.session_state.index is None:
        st.warning("å°šæœªå»ºç«‹çŸ¥è­˜åº«ã€‚è«‹åˆ‡åˆ°ã€Žå»ºç«‹çŸ¥è­˜åº«ï¼ˆè€å¸«ç”¨ï¼‰ã€ä¸Šå‚³æ•™æä¸¦å»ºç«‹ç´¢å¼•ã€‚")
    else:
        if "chat" not in st.session_state:
            st.session_state.chat = []

        prompt = st.chat_input("è¼¸å…¥ä½ çš„å•é¡Œï¼ˆä¾‹å¦‚ï¼šç‚ºä»€éº¼ COâ‚‚ çš„å°ç¨±ä¼¸ç¸®æ˜¯ Raman active è€Œ IR inactiveï¼Ÿï¼‰")
        if prompt:
            # retrieve
            blocks = retrieve(client, st.session_state.index, st.session_state.store, prompt, top_k=top_k, model=embed_model)

            # build messages
            context_text = "\n\n---\n".join(
                [f"[ä¾†æº {i+1}] ({b['meta']['source']} â€¢ {b['meta']['loc']})\n{b['text']}" for i, b in enumerate(blocks)]
            )
            style = "è«‹ä¿æŒå¹½é»˜å­¸å§Šé¢¨æ ¼ï¼Œç”¨ç°¡çŸ­æ¯”å–»æš–å ´ï¼Œ" if humor else "ä¿æŒä¸­æ€§å°ˆæ¥­èªžæ°£ï¼Œ"
            exam = "å­¸ç”Ÿéœ€è¦è€ƒå‰é‡é»žæ•´åˆï¼Œè«‹åœ¨æœ€å¾Œé™„ä¸Šã€Žè€ƒå‰å°æŠ„ã€çš„ä¸‰é»žè¦é»žï¼›" if exam_mode else "æœ€å¾Œç”¨ä¸€è¡Œã€Žè€ƒè©¦å°æé†’ã€çµå°¾ï¼›"
            sys = load_system_prompt() + "\n" + style + exam + "\né‡è¦ï¼šåªå…è¨±æ ¹æ“šä¸‹æ–¹ Context å…§å®¹å›žç­”ï¼›è‹¥ä¸è¶³è«‹å¦‚å¯¦èªªæ˜Žã€‚\n"
            messages = [
                {"role": "system", "content": sys},
                {"role": "user", "content": f"å•é¡Œï¼š{prompt}\n\nContextï¼š\n{context_text}"}
            ]

            with st.spinner("KMUCer æ€è€ƒä¸­â€¦"):
                resp = client.chat.completions.create(
                    model=model,
                    temperature=temperature,
                    messages=messages
                )
                answer = resp.choices[0].message.content

            st.session_state.chat.append({"role": "user", "content": prompt})
            st.session_state.chat.append({"role": "assistant", "content": answer, "context": blocks})

        # render
        for turn in st.session_state.get("chat", []):
            if turn["role"] == "user":
                with st.chat_message("user"):
                    st.markdown(turn["content"])
            else:
                with st.chat_message("assistant"):
                    st.markdown(turn["content"])
                    if "context" in turn:
                        with st.expander("ðŸ“Ž æœ¬æ¬¡å›žç­”åƒè€ƒä¾†æº"):
                            for b in turn["context"]:
                                st.write(f"- {b['meta']['source']}ï¼ˆ{b['meta']['loc']}ï¼‰")
